# Merge the two lists.
merged.list <- c(list1,list2)
# Print the merged list.
print(merged.list)
list1 <- list(1,2,3)
list2 <- list("Sun","Mon","Tue")
# Merge the two lists.
merged_list <- c(list1,list2)
# Print the merged list.
print(merged_list)
list1 <- list(1:5)
# Convert the lists to vectors.
v1 <- unlist(list1)
str(v1)
str(list1)
print(v1)
print(list1)
result <- v1+v2
print(result)
list1 <- list(1:5)
print(list1)
list2 <-list(10:14)
print(list2)
# Convert the lists to vectors.
v1 <- unlist(list1)
v2 <- unlist(list2)
print(v1)
print(v2)
# Now add the vectors
result <- v1+v2
print(result)
print(list1 + list2)
m <- matrix(c(1:12), nrow = 4, byrow = TRUE)
print(m)
m <- matrix(c(1:14), nrow = 4, byrow = TRUE)
print(m)
m <- matrix(c(1:12), nrow = 4, byrow = TRUE)
m <- matrix(c(1:12), nrow = 4, byrow = TRUE, dimnames = list(rownames, colnames))
print(m)
rownames = c("row1", "row2", "row3", "row4")
colnames = c("col1", "col2", "col3")
m <- matrix(c(1:12), nrow = 4, byrow = TRUE, dimnames = list(rownames, colnames))
print(m)
m2 <- matrix(c(1:12), nrow = 4, byrow = TRUE, dimnames = list(rownames, colnames))
print(m2)
print(m2[1][3])
print(m2[0][2])
rownames = c("row1", "row2", "row3", "row4")
colnames = c("col1", "col2", "col3")
m2 <- matrix(c(1:12), nrow = 4, byrow = TRUE, dimnames = list(rownames, colnames))
print(m2)
print(m2[1,3])
print(m2[1,])
print(m2[2,3])
print(m2[ ,3])
matrix1 <- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)
matrix2 <- matrix(c(5, 2, 0, 9, 3, 4), nrow = 2)
# Add the matrices.
result <- matrix1 + matrix2
print(result)
result <- matrix1 - matrix2
cat("Result of subtraction","\n")
print(result)
matrix1 <- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)
matrix2 <- matrix(c(5, 2, 0, 9, 3, 4), nrow = 2)
# Add the matrices.
result <- matrix1 + matrix2
cat("Result of addition","\n")
print(result)
# Subtract the matrices
result <- matrix1 - matrix2
cat("Result of subtraction","\n")
print(result)
# Multiply the matrices.
result <- matrix1 * matrix2
cat("Result of multiplication","\n")
print(result)
# Divide the matrices
result <- matrix1 / matrix2
cat("Result of division","\n")
print(result)
data <- c("East","West","East","North","North","East","West","West","West","East","North")
str(data)
data <- c("East","West","East","North","North","East","West","West","West","East","North")
# Apply the factor function.
factor_data <- factor(data)
print(factor_data)
new_order_data <- factor(factor_data,levels = c("East","West","North","South"))
print(new_order_data)
setwd("~/gitMachineLearning/1-preprocessing")
input_df <- read.csv('Data.csv', stringsAsFactors = FALSE)
input_df
input_df[is.na(input_df$Age), "Age"] <- mean(input_df$Age, na.rm = TRUE)
input_df
input_df[is.na(input_df$Age), "Age"] <- mean(input_df$Age, na.rm = TRUE)
input_df[is.na(input_df$Salary), "Salary"] <- mean(input_df, na.rm = TRUE)
input_df[is.na(input_df$Age), "Age"] <- mean(input_df$Age, na.rm = TRUE)
input_df[is.na(input_df$Salary), "Salary"] <- mean(input_df$Salary, na.rm = TRUE)
input_df
is.na(input_df$Salary)
input_df[1:2]
input_df[1,2]
str(input_df)
input_df["Country"]
input_df$Country <- as.factor(input_df$Country)
input_df$Purchased <- as.factor(input_df$Purchased)
input_df
str(input_df)
library(caTools)
ind <- sample.split(input_df, Y = input_df$Purchased, SplitRatio = .7)
train_data <- input_df[ind == TRUE, ]
test_data <- input_df[inde == FALSE, ]
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .7)
train_data <- input_df[ind == TRUE, ]
test_data <- input_df[inde == FALSE, ]
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .7)
train_data <- input_df[ind == TRUE, ]
test_data <- input_df[ind == FALSE, ]
View(train_data)
View(test_data)
setwd("~/gitMachineLearning/3-supervisedML/1-regression/1-simpleLinearRegression")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/1-simpleLinearRegression")
#Reading dataset
input_data <- read.csv("Salary_Data.csv")
#splitting data set in train and test
ind <- sample(2, nrow(input_data), prob = c(.7,.3), replace = TRUE)
train_data <- input_data[ind == 1, ]
test_data <- input_data[ind == 2, ]
#splitting data set in train and test
#install.packages("caTools")
library('caTools')
split <- sample.split(input_data$Salary, SplitRatio = .7)
train_data <- input_data[split == TRUE, ]
test_data <- input_data[split == FALSE, ]
#train model on training data
model <- lm(Salary~YearsExperience, data = train_data)
pred <- predict(model, newdata = test_data)
#plot
library(ggplot2)
ggplot() +
geom_point(aes(x = train_data$YearsExperience, y = train_data$Salary), colour = 'green') +
geom_line(aes(x = train_data$YearsExperience, y = predict(model, newdata = train_data)), colour = 'blue') +
ggtitle('Salary vs Year of experience in Training set.')
ggplot() +
geom_point(aes(x = test_data$YearsExperience, y = test_data$Salary), colour = 'green') +
geom_line(aes(x = train_data$YearsExperience, y = predict(model, newdata = train_data)), colour = 'blue') +
ggtitle('Salary vs Year of Experience in TEsting set.')
setwd("~/gitMachineLearning/3-supervisedML/1-regression/2-multipleLinearRegression")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/2-multipleLinearRegression")
input_df <- read.csv("50_Startups.csv", stringsAsFactors = FALSE)
#1. Categorical casting
input_df$State <- as.factor(input_df$State)
#2. divide in train test split
#install.packages("caTools")
library('caTools')
ind <- sample.split(Y = input_df$Profit, SplitRatio = .8)
train_data <- input_df[ind == TRUE, ]
test_data <- input_df[ind == FALSE, ]
#3. model building
model <- lm(train_data$Profit ~ ., data = train_data)
y_pred <- predict(model, newdata = test_data)
y_pred
input_df <- read.csv("50_startups.csv", stringsAsFactors = FALSE)
#1. Look for missing values.
table(is.na(input_df))
#2. Categorical casting
input_df$State <- as.factor(input_df$State)
#3. Train Test split
#install.packages('caTools')
library(caTools)
ind <- sample.split(input_df$Profit, SplitRatio = .8)
train_data <- input_df[ind == TRUE, ]
test_data <- input_df[ind == FALSE, ]
#4. building a model
model <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend + State, data = train_data)
summary(model)
#By looking in to summary of model it can be seen that state column is irrelevent. Because that is having highest p value among all the varibale. So remove state column an d rebuild our model.
model <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, data = train_data)
summary(model)
#Now Administration variable is having highest p value so remove that.
model <- lm(Profit ~ R.D.Spend + Marketing.Spend, data = train_data)
summary(model)
#Now Marketing.spend is having comparatively higher P value which is greter then significance level. (SL = .05). So remove that column as well.
model <- lm(Profit ~ R.D.Spend, data = train_data)
summary(model)
#Now model is ready. Because each variable is having p value less then significance level.
y_pred <- predict(model, newdata = test_data)
setwd("~/gitMachineLearning/3-supervisedML/1-regression/3-polynomialRegressor")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/3-polynomialRegressor")
input_df <- read.csv("Position_Salaries.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:3]
#linear model
lin_model <- lm(Salary~Level, data = input_df)
#polynomial model
input_df$Level2 <- input_df$Level^2
input_df$Level3 <- input_df$Level^3
input_df$Level4 <- input_df$Level^4
input_df$Level5 <- input_df$Level^5
pol_model <- lm(Salary~Level + Level2 + Level3 + Level4 + Level5, data = input_df)
#visualizing predictions
library(ggplot2)
ggplot()+
geom_point(aes(x = input_df$Level, y = input_df$Salary), colour = 'red') +
geom_line(aes(x = input_df$Level, y = predict(lin_model, newdata = input_df)), colour = 'blue') +
ggtitle("Actual vs Fitted line.") +
xlab('Level') +
ylab('Salary')
ggplot()+
geom_point(aes(x = input_df$Level, y = input_df$Salary), colour = 'red') +
geom_line(aes(x = input_df$Level, y = predict(pol_model, input_df)), colour = 'black') +
ggtitle("Actual vs fitted polynomial model") +
xlab('Level') +
ylab('Salary')
#predict
pred <- predict(pol_model, newdata = data.frame(Level = 6.5, Level2 = 6.5^2, Level3 = 6.5^3, Level4 = 6.5^4, Level5 = 6.5^5))
pred
setwd("~/gitMachineLearning/3-supervisedML/1-regression/4-supportVectorRegressor")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/4-supportVectorRegressor")
input_df <- read.csv("Position_Salaries.csv")
input_df <- input_df[2:3]
#building model
#install.library("e1071")
library(e1071)
model <- svm(formula = Salary ~ Level, data = input_df, type = "eps-regression")
library(ggplot2)
ggplot()+
geom_point(aes(x = input_df$Level, y = input_df$Salary), color = 'red') +
geom_line(aes(x = input_df$Level, y = predict(model, input_df)), color = 'black') +
ggtitle("Actual Vs predicted") +
xlab("Level") +
ylab("Salary")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/5-decisionTreeRegressor")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/5-decisionTreeRegressor")
input_df <- read.csv("Position_Salaries.csv")
input_df <- input_df[2:3]
#building model
#install.packages("rpart")
library(rpart)
model <- rpart(formula = Salary ~ Level, data = input_df, control = rpart.control(minsplit = 1))
ggplot()+
geom_point(aes(x = input_df$Level, y = input_df$Salary), color = 'red') +
geom_line(aes(x = input_df$Level, y = predict(model, input_df)), color = 'black') +
ggtitle("Actual Vs predicted") +
xlab("Level") +
ylab("Salary")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/6-randomForestRegressor")
setwd("~/gitMachineLearning/3-supervisedML/1-regression/6-randomForestRegressor")
input_df <- read.csv("Position_Salaries.csv")
input_df <- input_df[2:3]
#building model
# install.packages('randomForest')
library(randomForest)
set.seed(1234)
model <- randomForest(x = input_df[1],
y = input_df[, 2], ntree = 500)
pred <- predict(model, newdata = data.frame(Level = 6.5))
#Visualization
#For continuos graph
sequence <- seq(min(input_df$Level), max(input_df$Level), by = .01)
ggplot()+
geom_point(aes(x = input_df$Level, y = input_df$Salary), color = 'red') +
geom_line(aes(x = sequence, y = predict(model, newdata = data.frame(Level = sequence))), color = 'black') +
ggtitle("Actual Vs predicted") +
xlab("Level") +
ylab("Salary")
setwd("~/gitMachineLearning/3-supervisedML/2-classification/1-logisticRegression")
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender,
levels = c('Male', 'Female'),
labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
#model building
model <- glm(formula = Purchased ~ . ,
data = train_set,
family = 'binomial')
prob_preds <- predict( model, newdata = test_set[-4], type = 'response')
y_pred <- ifelse(prob_preds > 0.5, 1, 0)
#Evaluation by confusion matrix
cm <- table(y_pred, test_set$Purchased)
cm
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender, levels = c('Male', 'Female'), labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
#model building
library(class)
ypred <- knn(train = train_set[, -4], test = test_set[, -4], k = 5, cl = train_set[, 4])
#Evaluation by confusion matrix
cm <- table(ypred, test_set$Purchased)
cm
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender,
levels = c('Male', 'Female'),
labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
#model building
#install.packages("e1071")
library(e1071)
model <- svm(formula = Purchased ~ . ,
data = train_set,
type = 'C-classification', # Default type for classification is C-classification.
kernel = "linear")
y_pred <- predict(model, newdata = test_set )
#Evaluation by confusion matrix
cm <- table(y_pred, test_set$Purchased)
cm
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender,
levels = c('Male', 'Female'),
labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
#model building
#install.packages("e1071")
library(e1071)
model <- svm(formula = Purchased ~ . ,
data = train_set,
type = 'C-classification', # Default type for classification is C-classification.
kernel = "radial")
y_pred <- predict(model, newdata = test_set )
#Evaluation by confusion matrix
cm <- table(y_pred, test_set$Purchased)
cm
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender,
levels = c('Male', 'Female'),
labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
#model building
#install.packages("e1071")
library(e1071)
model <- naiveBayes(formula = Purchased ~ ., data = train_set)
y_pred <- predict(model, newdata = test_set )
#Evaluation by confusion matrix
cm <- table(y_pred, test_set$Purchased)
cm
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender,
levels = c('Male', 'Female'),
labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
#model building
#install.packages("e1071")
library(rpart)
model <- rpart(formula = Purchased ~ .,
data = train_set)
y_pred <- predict(model, newdata = test_set, type = 'class')
#Evaluation by confusion matrix
cm <- table(y_pred, test_set$Purchased)
cm
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender,
levels = c('Male', 'Female'),
labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
#model building
#install.packages("e1071")
library(randomForest)
model <- randomForest(x = train_set[-4],
y = train_set[, 4],
ntree = 500)
y_pred <- predict(model, newdata = test_set )
#Evaluation by confusion matrix
cm <- table(y_pred, test_set$Purchased)
cm
setwd("~/Dipesh/study/machineLearning/udemy/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 5 - Multiple Linear Regression")
input_df <- read.csv("50_startups.csv", stringsAsFactors = FALSE)
#1. Look for missing values.
table(is.na(input_df))
#2. Categorical casting
input_df$State <- as.factor(input_df$State)
#3. Train Test split
#install.packages('caTools')
library(caTools)
ind <- sample.split(input_df$Profit, SplitRatio = .8)
train_data <- input_df[ind == TRUE, ]
test_data <- input_df[ind == FALSE, ]
#4. building a model
model <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend + State, data = train_data)
summary(model)
difference <- y_pred - test_data$Profit
setwd("~/gitMachineLearning/3-supervisedML/1-regression/2-multipleLinearRegression")
input_df <- read.csv("50_Startups.csv", stringsAsFactors = FALSE)
#1. Categorical casting
input_df$State <- as.factor(input_df$State)
#2. divide in train test split
#install.packages("caTools")
library('caTools')
ind <- sample.split(Y = input_df$Profit, SplitRatio = .8)
train_data <- input_df[ind == TRUE, ]
test_data <- input_df[ind == FALSE, ]
#3. model building
model <- lm(train_data$Profit ~ ., data = train_data)
y_pred <- predict(model, newdata = test_data)
#4. Error calculations
#RMSE
difference <- y_pred - test_data$Profit
difference
rmse <- sqrt(mean(difference^2))
rmse
difference <- test_data$Profit - y_pred
rmse <- sqrt(mean(difference^2))
rmse
mape <- mean(abs(difference/test_data$Profit)) * 100
mape
mean(difference^2)
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender,
levels = c('Male', 'Female'),
labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
train_set[-4]
setwd("~/gitMachineLearning/3-supervisedML/2-classification/")
input_df <- read.csv("Social_Network_Ads.csv", stringsAsFactors = FALSE)
input_df <- input_df[2:5]
#Categorical casting
input_df$Gender <- as.numeric(factor(input_df$Gender, levels = c('Male', 'Female'), labels = c(1,2)))
input_df$Purchased <- as.factor(input_df$Purchased)
#Splitting in train and test set
library(caTools)
ind <- sample.split(input_df$Purchased, SplitRatio = .8)
train_set <- input_df[ind == TRUE, ]
test_set <- input_df[ind == FALSE, ]
#Feature scaling
train_set[-4] <- scale(train_set[-4])
test_set[-4] <- scale(test_set[-4])
ypred <- knn(train = train_set[, -4], test = test_set[, -4], k = 5, cl = train_set[, 4])
